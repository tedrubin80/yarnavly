# docker-compose.yml - Complete deployment stack
version: '3.8'

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: yarn_management_db
    environment:
      POSTGRES_DB: yarn_management
      POSTGRES_USER: yarn_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - yarn_network

  # Redis for caching and sessions
  redis:
    image: redis:7-alpine
    container_name: yarn_management_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - yarn_network

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: yarn_management_api
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://yarn_user:${DB_PASSWORD}@postgres:5432/yarn_management
      REDIS_URL: redis://redis:6379
      JWT_SECRET: ${JWT_SECRET}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
      GOOGLE_REDIRECT_URI: ${GOOGLE_REDIRECT_URI}
      RAVELRY_CLIENT_ID: ${RAVELRY_CLIENT_ID}
      RAVELRY_CLIENT_SECRET: ${RAVELRY_CLIENT_SECRET}
      FRONTEND_URL: ${FRONTEND_URL}
      PORT: 3001
    ports:
      - "3001:3001"
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - yarn_network
    volumes:
      - ./uploads:/app/uploads
      - ./logs:/app/logs

  # Frontend Application
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        REACT_APP_API_URL: ${REACT_APP_API_URL}
        REACT_APP_ADMIN_URL: ${REACT_APP_ADMIN_URL}
    container_name: yarn_management_frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - yarn_network

  # Admin Panel
  admin:
    build:
      context: ./admin
      dockerfile: Dockerfile
      args:
        REACT_APP_API_URL: ${REACT_APP_API_URL}
    container_name: yarn_management_admin
    ports:
      - "3002:80"
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - yarn_network

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: yarn_management_nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - ./uploads:/var/www/uploads
    depends_on:
      - frontend
      - backend
      - admin
    restart: unless-stopped
    networks:
      - yarn_network

  # Backup Service
  backup:
    build:
      context: ./backup
      dockerfile: Dockerfile
    container_name: yarn_management_backup
    environment:
      DATABASE_URL: postgresql://yarn_user:${DB_PASSWORD}@postgres:5432/yarn_management
      BACKUP_SCHEDULE: "0 2 * * *"  # Daily at 2 AM
      RETENTION_DAYS: 30
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      S3_BUCKET: ${S3_BACKUP_BUCKET}
    volumes:
      - ./backups:/app/backups
      - postgres_data:/var/lib/postgresql/data:ro
    depends_on:
      - postgres
    restart: unless-stopped
    networks:
      - yarn_network

volumes:
  postgres_data:
  redis_data:

networks:
  yarn_network:
    driver: bridge

---

# .env.example - Environment variables template
# Database Configuration
DB_PASSWORD=your_secure_database_password
DATABASE_URL=postgresql://yarn_user:password@localhost:5432/yarn_management

# JWT Configuration
JWT_SECRET=your_super_secret_jwt_key_here_make_it_long_and_random

# Google Drive API
GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret
GOOGLE_REDIRECT_URI=http://localhost:3001/api/auth/google/callback

# Ravelry API
RAVELRY_CLIENT_ID=your_ravelry_username
RAVELRY_CLIENT_SECRET=your_ravelry_password

# Frontend URLs
FRONTEND_URL=http://localhost:3000
REACT_APP_API_URL=http://localhost:3001/api
REACT_APP_ADMIN_URL=http://localhost:3002

# Backup Configuration
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
S3_BACKUP_BUCKET=yarn-management-backups

# Email Configuration (optional)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@gmail.com
SMTP_PASS=your_app_password

# Redis Configuration
REDIS_URL=redis://localhost:6379

---

# backend/Dockerfile
FROM node:18-alpine

WORKDIR /app

# Copy package files
COPY package*.json ./
RUN npm ci --only=production

# Copy source code
COPY . .

# Create necessary directories
RUN mkdir -p uploads logs

# Expose port
EXPOSE 3001

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3001/health || exit 1

# Start the application
CMD ["npm", "start"]

---

# frontend/Dockerfile
FROM node:18-alpine as build

WORKDIR /app

# Copy package files
COPY package*.json ./
RUN npm ci

# Copy source code and build
COPY . .
ARG REACT_APP_API_URL
ARG REACT_APP_ADMIN_URL
ENV REACT_APP_API_URL=$REACT_APP_API_URL
ENV REACT_APP_ADMIN_URL=$REACT_APP_ADMIN_URL

RUN npm run build

# Production stage
FROM nginx:alpine

# Copy built files
COPY --from=build /app/build /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port
EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]

---

# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:3001;
    }

    upstream frontend {
        server frontend:80;
    }

    upstream admin {
        server admin:80;
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=uploads:10m rate=1r/s;

    # Main application
    server {
        listen 80;
        server_name localhost;
        client_max_body_size 100M;

        # Frontend
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # API routes
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts for file uploads
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        # File uploads with rate limiting
        location /api/upload/ {
            limit_req zone=uploads burst=5 nodelay;
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Extended timeouts for large files
            proxy_connect_timeout 120s;
            proxy_send_timeout 120s;
            proxy_read_timeout 120s;
        }

        # Static file serving
        location /uploads/ {
            alias /var/www/uploads/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }

    # Admin panel
    server {
        listen 80;
        server_name admin.localhost;

        location / {
            proxy_pass http://admin;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
}

---

# backup/Dockerfile
FROM postgres:15-alpine

RUN apk add --no-cache \
    aws-cli \
    curl \
    dcron \
    bash

WORKDIR /app

COPY backup-script.sh /app/
COPY crontab /etc/crontabs/root

RUN chmod +x /app/backup-script.sh

CMD ["crond", "-f", "-d", "8"]

---

# backup/backup-script.sh
#!/bin/bash

set -e

# Configuration
BACKUP_DIR="/app/backups"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
BACKUP_FILE="yarn_management_backup_${TIMESTAMP}.sql"
RETENTION_DAYS=${RETENTION_DAYS:-30}

# Create backup directory
mkdir -p $BACKUP_DIR

echo "Starting backup at $(date)"

# Create database dump
pg_dump $DATABASE_URL > "$BACKUP_DIR/$BACKUP_FILE"

# Compress backup
gzip "$BACKUP_DIR/$BACKUP_FILE"
COMPRESSED_FILE="${BACKUP_FILE}.gz"

echo "Backup created: $COMPRESSED_FILE"

# Upload to S3 if configured
if [ ! -z "$S3_BUCKET" ] && [ ! -z "$AWS_ACCESS_KEY_ID" ]; then
    echo "Uploading to S3..."
    aws s3 cp "$BACKUP_DIR/$COMPRESSED_FILE" "s3://$S3_BUCKET/database-backups/$COMPRESSED_FILE"
    echo "Upload completed"
fi

# Clean up old backups locally
find $BACKUP_DIR -name "*.gz" -mtime +$RETENTION_DAYS -delete

# Clean up old S3 backups
if [ ! -z "$S3_BUCKET" ] && [ ! -z "$AWS_ACCESS_KEY_ID" ]; then
    CUTOFF_DATE=$(date -d "$RETENTION_DAYS days ago" +%Y-%m-%d)
    aws s3 ls "s3://$S3_BUCKET/database-backups/" | while read -r line; do
        FILE_DATE=$(echo $line | awk '{print $1}')
        FILE_NAME=$(echo $line | awk '{print $4}')
        if [[ "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
            aws s3 rm "s3://$S3_BUCKET/database-backups/$FILE_NAME"
            echo "Deleted old backup: $FILE_NAME"
        fi
    done
fi

echo "Backup completed at $(date)"

---

# backup/crontab
# Daily backup at 2 AM
0 2 * * * /app/backup-script.sh >> /var/log/backup.log 2>&1

---

# scripts/deploy.sh
#!/bin/bash

set -e

echo "🚀 Starting deployment..."

# Check if .env file exists
if [ ! -f .env ]; then
    echo "❌ .env file not found. Please copy .env.example to .env and configure it."
    exit 1
fi

# Load environment variables
source .env

# Build and start services
echo "🏗️ Building and starting services..."
docker-compose down
docker-compose build --no-cache
docker-compose up -d

# Wait for database to be ready
echo "⏳ Waiting for database to be ready..."
timeout 60 bash -c 'until docker-compose exec postgres pg_isready -U yarn_user; do sleep 1; done'

# Run database migrations
echo "🗄️ Running database migrations..."
docker-compose exec backend npm run migrate

# Check service health
echo "🏥 Checking service health..."
sleep 10

# Check if all services are running
services=("postgres" "redis" "backend" "frontend" "admin" "nginx")
for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "✅ $service is running"
    else
        echo "❌ $service is not running"
        docker-compose logs $service
        exit 1
    fi
done

# Test API endpoint
if curl -f http://localhost/api/health > /dev/null 2>&1; then
    echo "✅ API health check passed"
else
    echo "❌ API health check failed"
    exit 1
fi

echo "🎉 Deployment completed successfully!"
echo ""
echo "Access points:"
echo "- Main application: http://localhost"
echo "- Admin panel: http://admin.localhost (add to /etc/hosts if needed)"
echo "- API: http://localhost/api"
echo ""
echo "To view logs: docker-compose logs -f [service-name]"
echo "To stop: docker-compose down"

---

# scripts/setup.sh
#!/bin/bash

set -e

echo "🎯 Setting up Yarn Management System..."

# Check if Docker is installed
if ! command -v docker &> /dev/null; then
    echo "❌ Docker is not installed. Please install Docker first."
    exit 1
fi

# Check if Docker Compose is installed
if ! command -v docker-compose &> /dev/null; then
    echo "❌ Docker Compose is not installed. Please install Docker Compose first."
    exit 1
fi

# Create .env file if it doesn't exist
if [ ! -f .env ]; then
    echo "📝 Creating .env file..."
    cp .env.example .env
    
    # Generate JWT secret
    JWT_SECRET=$(openssl rand -base64 64 | tr -d "=+/" | cut -c1-64)
    sed -i "s/your_super_secret_jwt_key_here_make_it_long_and_random/$JWT_SECRET/g" .env
    
    # Generate database password
    DB_PASSWORD=$(openssl rand -base64 32 | tr -d "=+/" | cut -c1-32)
    sed -i "s/your_secure_database_password/$DB_PASSWORD/g" .env
    
    echo "✅ .env file created with generated secrets"
    echo "⚠️  Please edit .env file and configure your API keys:"
    echo "   - Google Drive API credentials"
    echo "   - Ravelry API credentials"
    echo "   - Email settings (optional)"
fi

# Create necessary directories
echo "📁 Creating directories..."
mkdir -p uploads logs backups nginx/ssl

# Set permissions
chmod 755 uploads logs backups
chmod +x scripts/deploy.sh scripts/backup.sh

echo "✅ Setup completed!"
echo ""
echo "Next steps:"
echo "1. Edit .env file with your API credentials"
echo "2. Run: ./scripts/deploy.sh"
echo "3. Access the application at http://localhost"

---

# Health check and monitoring
# scripts/health-check.sh
#!/bin/bash

set -e

echo "🏥 Running health checks..."

# Check if services are running
services=("postgres" "redis" "backend" "frontend" "admin" "nginx")
failed_services=()

for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "✅ $service is healthy"
    else
        echo "❌ $service is down"
        failed_services+=($service)
    fi
done

# Check API endpoints
endpoints=(
    "http://localhost/api/health"
    "http://localhost/"
)

for endpoint in "${endpoints[@]}"; do
    if curl -f "$endpoint" > /dev/null 2>&1; then
        echo "✅ $endpoint is responding"
    else
        echo "❌ $endpoint is not responding"
        failed_services+=("api")
    fi
done

# Check disk usage
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt 90 ]; then
    echo "⚠️  Disk usage is at ${DISK_USAGE}%"
fi

# Check database connection
if docker-compose exec postgres pg_isready -U yarn_user > /dev/null 2>&1; then
    echo "✅ Database connection is healthy"
else
    echo "❌ Database connection failed"
    failed_services+=("database")
fi

if [ ${#failed_services[@]} -eq 0 ]; then
    echo "🎉 All health checks passed!"
    exit 0
else
    echo "❌ Failed services: ${failed_services[*]}"
    exit 1
fi